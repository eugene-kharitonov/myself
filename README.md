# Eugene Kharitonov ðŸ‘‹

---

## About Me
[![Google Scholar](https://img.shields.io/badge/Google_Scholar-4285F4?style=for-the-badge&logo=google-scholar&logoColor=white)](https://scholar.google.com/citations?user=8PE1wjQAAAAJ&hl=en)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/eugene-kharitonov-29415214)
[![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/n0mad_0)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/eugene-kharitonov)

<img src="photo.png" alt="My Photo" height="200">


Hi there! I am currently working at [Kyutai](https://kyutai.org) (a non-profit open-research lab) as a Technical Staff.

Before, I was a Research Scientist at Google DeepMind (coming from Google Brain) and worked as a Research Engineer at Facebook AI Research (FAIR).

My current research is focused on **speech and speech-text LLMs**. I co-first-authored the [very first paper](https://aclanthology.org/2021.tacl-1.79/) that applied Transformer-based generative language modeling on quantized speech representations.
Fast-forward a few short years and this idea became one of the mainstream approaches for generative audio and ended up powering speech capabilities of the modern industrial-scale LLMs such as Gemini.


Even earlier, I used to work on understanding the internal workings of deep learning models, studied communicating neural agents, dipped my toes in Federated Learning and a few other exciting topics!

I earned my PhD at the University of Glasgow.

